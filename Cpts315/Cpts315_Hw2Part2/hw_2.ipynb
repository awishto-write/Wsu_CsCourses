{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcJK3kXl--c3"
      },
      "source": [
        "# **CPT_S 315 HW2-Coding: Apriori Algorithm (35 points)**\n",
        "In this homework, you are required to implement the apriori algorithm and return valid association rules based on the output frequent itemset from you algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRbrJumjohIe"
      },
      "source": [
        "**Product Recommendations:**\n",
        "\n",
        "The action or practice of selling additional products or services to existing customers is called cross-selling. Giving product recommendation is one of the examples of cross-selling that are frequently used by online retailers. One simple method to give product recommendations is to recommend products that are frequently browsed together by the customers.\n",
        "\n",
        "Suppose we want to recommend new products to the customer based on the products they have already browsed on the online website. Write a program using the A-priori algorithm to find products which are frequently browsed together. Fix the support to s =100 (i.e., product pairs need to occur together at least 100 times to be considered frequent) and  nd itemsets of size 2, 3 and 4.\n",
        "\n",
        "Use the online browsing behavior dataset provided with this homework. Each line represents a browsing session of a customer. On each line, each string of 8 characters represents the id of an item browsed during that session. The items are separated by spaces.\n",
        "\n",
        "Our aim is to:\n",
        "\n",
        "**a)** Identify pairs of items (X, Y ) such that the support of {X,Y} is at least 100. For all such pairs, compute the confidence scores of the corresponding association rules: X => Y , Y => X. Sort the rules in decreasing order of con dence scores and list the top 5 rules in the writeup. Break ties, if any, by lexicographically1 increasing order on the left hand side of the rule.\n",
        "\n",
        "**b)** Identify items triples (X, Y, Z ) such that the support of {X,Y,Z} is at least 100. For all such triples, compute the confidence scores of the corresponding association rules: (X,Y) => Z , (X,Z) => Y, (Y,Z) => X. Sort the rules in decreasing order of con dence scores and list the top 5 rules in the writeup. Break ties, if any, by lexicographically1 increasing order of the first then second item pair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jEPzK3jM_hD"
      },
      "source": [
        "### **Requirements:**\n",
        "Please download the ``files hw_2.ipynb``, ``test-dataset.txt``, and ``browsing-dataset`` from Canvas and complete the codes in hw_2.ipynb following the instructions in the file. You can run and debug your codes using Google Colab. After completing the tasks, save hw_2.ipynb as hw_2.py by clicking File -> Download -> Download .py. Then upload hw_2.py to Gradescope. The autograder on Gradescope will automatically grade your coding homework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 537,
      "metadata": {
        "id": "d6wLqKYS8N_5"
      },
      "outputs": [],
      "source": [
        "#We will provide two sample of testing dataset for your debugging.\n",
        "# The testing dataset used on Gradescope will be different but with the same structure.\n",
        "# Upload the file \"browsing-dataset.txt\" and \"test-data.txt\"\n",
        "# Select the two files and upload them together or You can run this cell twice to upload one by one\n",
        "#These two dataset will be used for testing the outputs of your aprior algorithm\n",
        "# These two files can be downloaded on Canvas\n",
        "try:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "except ImportError as e:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbjggZlM-D1L"
      },
      "source": [
        " Your current working directory should include the following filenames from the assignment:\n",
        "\n",
        "```\n",
        "['browsing-dataset.txt', 'test-data.txt', 'Apriori_HW2_v3_solution']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc83ETI1a3o9"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In some cells and files you will see code blocks that look like this:\n",
        "\n",
        "```python\n",
        "##############################################################################\n",
        "#                    TODO: Write the equation for a line                     #\n",
        "##############################################################################\n",
        "raise NotImplementedError()\n",
        "##############################################################################\n",
        "#                              END OF YOUR CODE                              #\n",
        "##############################################################################\n",
        "```\n",
        "\n",
        "You should replace the \"raise NotImplementedError()\"  with your own code and leave the blocks intact, like this:\n",
        "\n",
        "```python\n",
        "##############################################################################\n",
        "#                    TODO: Write the equation for a line                     #\n",
        "##############################################################################\n",
        "y = m * x + b\n",
        "##############################################################################\n",
        "#                              END OF YOUR CODE                              #\n",
        "##############################################################################\n",
        "```\n",
        "\n",
        "# **When completing the notebook, please adhere to the following rules:**\n",
        "## - **Do not write or modify any code outside of code blocks**\n",
        "## - **Do not add or delete any cells from the notebook. You may add new cells to perform scatch work, but delete them before submitting.**\n",
        "## - **Make sure at least you pass all the testing cases before submitting**.\n",
        "\n",
        "\n",
        "\n",
        "# **You will only get credit for code that has been run!**.\n",
        "\n",
        "# **We will not re-run your notebook -- you will get your credits automatically on Gradescope**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 538,
      "metadata": {
        "id": "Jjeirlcv5th_"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "from itertools import combinations, chain\n",
        "from numpy import tri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB-edGEjbTeW"
      },
      "source": [
        "**Your Answer is needed in the following cell:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 539,
      "metadata": {
        "id": "KEY-127sOLzA"
      },
      "outputs": [],
      "source": [
        "# New\n",
        "def generatecandidates(Lk_prev, k):\n",
        "    candidates = []\n",
        "    len_Lk = len(Lk_prev)\n",
        "    for i in range(len_Lk):\n",
        "        for j in range(i + 1, len_Lk):\n",
        "            #Check if L1 and L2 match:\n",
        "            L1 = sorted(list(Lk_prev[i])[:k - 2])\n",
        "            L2 = sorted(list(Lk_prev[j])[:k - 2])\n",
        "            if L1 == L2:\n",
        "                # insert to list:\n",
        "                candidates.append(sorted(list(set(Lk_prev[i]) | set(Lk_prev[j]))))\n",
        "    return candidates\n",
        "\n",
        "#APRIORI Alg:\n",
        "def apriori(trans_data, min_sup, K_max):\n",
        "    item_counts = {}\n",
        "    #For each transaction:\n",
        "    for transaction in trans_data:\n",
        "        for item in transaction:\n",
        "            if item in item_counts:\n",
        "                item_counts[item] += 1\n",
        "            else:\n",
        "                item_counts[item] = 1\n",
        "\n",
        "    # get frequent items:\n",
        "    frequent_items = [[] for _ in range(K_max)]\n",
        "\n",
        "    #get first list, and replacement list\n",
        "    L1 = [[item] for item, count in item_counts.items() if count >= min_sup]\n",
        "    unsorted = [item for item, count in item_counts.items() if count >= min_sup]\n",
        "    frequent_items[0] = sorted(L1)\n",
        "\n",
        "    k = 2\n",
        "    # go for length:\n",
        "   # while len(frequent_items[k - 2]) > 0 and k <= K_max:\n",
        "    while k <= K_max:\n",
        "        # generate candidates:\n",
        "        Ck = generatecandidates(frequent_items[k - 2], k)\n",
        "        item_counts = {tuple(candidate): 0 for candidate in Ck}\n",
        "        #for each transaction get the subset\n",
        "        for transaction in trans_data:\n",
        "            subsets = [tuple(sorted(comb)) for comb in combinations(transaction, k)] # Convert sets to tuples\n",
        "            for subset in subsets:\n",
        "                if subset in item_counts:\n",
        "                    item_counts[subset] += 1\n",
        "\n",
        "        Lk = [list(candidate) for candidate, count in item_counts.items() if count >= min_sup]\n",
        "        frequent_items[k - 1] = sorted(Lk)\n",
        "        k += 1\n",
        "        \n",
        "    #insert replacement list\n",
        "    frequent_items[0] = unsorted\n",
        "        \n",
        "    return frequent_items\n",
        "\n",
        "# New\n",
        "\n",
        "# def apriori_pass_1(data_set, s):\n",
        "\n",
        "#     item_counts = {}\n",
        "\n",
        "#     for basket in data_set:\n",
        "#         for item in basket:\n",
        "#             if item_counts.get(item):\n",
        "#                 item_counts[item] = item_counts[item] + 1\n",
        "#             else:\n",
        "#                 item_counts[item] = 1\n",
        "\n",
        "#     #return {key: value for (key, value) in item_counts.items() if value >= s}\n",
        "    \n",
        "#     frequent_itemsets = {key: value for (key, value) in item_counts.items() if value >= s}\n",
        "#     print(\"Frequent Itemsets:\", frequent_itemsets)\n",
        "#     return frequent_itemsets\n",
        "\n",
        "# def generate_combinations(items, r):\n",
        "#     \"\"\"Generate combinations of items with length r as lists.\"\"\"\n",
        "#     if r == 0:\n",
        "#         yield []\n",
        "#     else:\n",
        "#         items = list(items)  # Convert the slice to a list\n",
        "#         for i in range(len(items)):\n",
        "#             for combo in generate_combinations(items[i+1:], r-1):\n",
        "#                 yield [items[i]] + combo\n",
        "\n",
        "# def apriori_pass_2(frequent_items, data_set, s):\n",
        "#     item_counts = {}\n",
        "#     line_count = 0\n",
        "#     candidates = set(combinations(frequent_items, 2))\n",
        "#     print(\"Candidates:\", candidates)\n",
        "\n",
        "#     for line in data_set:\n",
        "#         for candidate in candidates:\n",
        "#             if candidate[0] in line and candidate[1] in line:\n",
        "#                 if item_counts.get(candidate):\n",
        "#                     item_counts[candidate] += 1\n",
        "#                 else:\n",
        "#                     item_counts[candidate] = 1\n",
        "#         line_count += 1\n",
        "        \n",
        "#     print(\"Item Counts:\", item_counts)\n",
        "    \n",
        "#     frequent_itemsets = {key: value for (key, value) in item_counts.items() if value >= s}\n",
        "#     print(\"Frequent Itemsets:\", frequent_itemsets)\n",
        "#     return frequent_itemsets\n",
        "\n",
        "# def create_triplets(lst):\n",
        "#     triplets = []\n",
        "#     for i in range(len(lst)-2):\n",
        "#         triplet = lst[i:i+3]\n",
        "#         triplets.append(triplet)\n",
        "#     return triplets\n",
        "\n",
        "\n",
        "# def create_triplets(lst):\n",
        "#     triplets = []\n",
        "#     for i in range(len(lst)-2):\n",
        "#         triplet = [item for sublist in lst[i:i+3] for item in sublist]\n",
        "#        # triplet = [item for sublist in lst[i:i] for item in sublist]\n",
        "#         triplets.append(triplet)\n",
        "#     return triplets\n",
        "\n",
        "# def create_triplets(lst):\n",
        "#     triplets = []\n",
        "#     for itemset in lst:\n",
        "#         # Generate combinations of three items from the itemset\n",
        "#         for triplet in combinations(itemset, 3):\n",
        "#             triplets.append(list(triplet))\n",
        "#     return triplets\n",
        "\n",
        "# def create_triplets(lst):\n",
        "#     triplets = []\n",
        "#     for i in range(len(lst)-3):\n",
        "#         triplet = [lst[i], lst[i+1], lst[i+2]]\n",
        "#         triplets.append(triplet)\n",
        "#     return triplets\n",
        "\n",
        "\n",
        "# def apriori_pass_3(frequent_items, data_set, s):\n",
        "#     item_counts = {}\n",
        "#     line_count = 0\n",
        "#    # triples = list(itertools.combinations(frequent_items,3)) \n",
        "    \n",
        "#     # triples = combinations(frequent_items, 3)\n",
        "#     frequent_items = [list(itemset) for itemset in frequent_items]\n",
        "#     triples = create_triplets(frequent_items)\n",
        "#     #triples = list(itertools.combinations(frequent_items,3)) \n",
        "    \n",
        "#     # print(\"Triples: \",  triples)\n",
        "    \n",
        "#     # # Generate triples\n",
        "#     # triples = []\n",
        "#     # n = len(frequent_items)\n",
        "#     # for i in range(n):\n",
        "#     #     for j in range(i+1, n):\n",
        "#     #         for k in range(j+1, n):\n",
        "#     #             triples.append([frequent_items[i], frequent_items[j], frequent_items[k]])\n",
        "    \n",
        "#     print(\"Triples: \",  triples)\n",
        "\n",
        "#     for line in data_set:\n",
        "#         for combination in triples:\n",
        "#             if all(item in line for item in combination):\n",
        "#                 if item_counts.get(combination):\n",
        "#                     item_counts[combination] += 1\n",
        "#                 else:\n",
        "#                     item_counts[combination] = 1\n",
        "#         line_count += 1\n",
        "        \n",
        "#     # for line in data_set:\n",
        "#     #     for combination in triples:\n",
        "#     #        # if all(item in line for item in combination):\n",
        "#     #         if combination[0] in line and combination[1] in line and combination[2] in line:\n",
        "#     #             if item_counts.get(combination):\n",
        "#     #                 item_counts[combination] += 1\n",
        "#     #             else:\n",
        "#     #                 item_counts[combination] = 1\n",
        "#     #     line_count += 1\n",
        "    \n",
        "#     frequent_itemsets = {key: value for (key, value) in item_counts.items() if value >= s}\n",
        "#     print(\"Frequent Itemsets:\", frequent_itemsets)\n",
        "#     return frequent_itemsets\n",
        "#     #return {key: value for (key, value) in item_counts.items() if value >= s}\n",
        "    \n",
        "# def apriori_pass_4(frequent_items, data_set, min_sup):\n",
        "#     item_counts = {}\n",
        "#     line_count = 0\n",
        "#     quadruples = set(combinations(frequent_items, 4))\n",
        "\n",
        "#     for line in data_set:\n",
        "#         for combination in quadruples:\n",
        "#             if all(item in line for item in combination):\n",
        "#                 if item_counts.get(combination):\n",
        "#                     item_counts[combination] += 1\n",
        "#                 else:\n",
        "#                     item_counts[combination] = 1\n",
        "#         line_count += 1\n",
        "    \n",
        "#     frequent_itemsets = {key: value for (key, value) in item_counts.items() if value >= min_sup}\n",
        "#     return frequent_itemsets\n",
        "\n",
        "# def reorder_itemset(itemset):\n",
        "#     \"\"\"\n",
        "#     Reorder the itemset based on the lexicographical order of the items.\n",
        "#     \"\"\"\n",
        "#     return sorted(itemset)\n",
        "\n",
        "# def apriori(trans_data : list, min_sup: int, K_max: int) -> list:\n",
        "    \"\"\"\n",
        "    Please include your own implementation of the apriori algorithm.\n",
        "    frequent_items is the output which is a list of frequent itemsets\n",
        "    The input of the function:\n",
        "    trans_data: (tpye:list) contains the transition dataset, for example, if we use the ''test-data.txt'' as input, then the trans_data is:\n",
        "    [['I1', 'I2', 'I5'], ['I2', 'I4'], ['I2', 'I4'], ['I1', 'I2', 'I4'], ['I1', 'I3'], ['I2', 'I3'], ['I1', 'I3'], ['I1', 'I2', 'I3', 'I5'], ['I1', 'I2', 'I3']]\n",
        "    You will find how to transform the 'txt' data into a list in the testing cell.\n",
        "    min_sup: minimum support count, pay attention, this is for the support count not for support(relative support)\n",
        "\n",
        "    K_max: indicate the largest size of frequent itemsets for the output. For example, if K_max = 3, then your algorithm will out put a list of frequent itemsets:\n",
        "    [[L_1],[L_2],[L_3]], where L_k represents the set of frequent itemset with size k.\n",
        "    For example, for the ''test-data.txt'' with min_sup = 2, and K_max = 3, the output is:\n",
        "    frequent_items = [['I2', 'I1', 'I5', 'I4', 'I3'],[['I1', 'I2'], ['I1', 'I3'], ['I1', 'I5'], ['I2', 'I3'], ['I2', 'I4'], ['I2', 'I5']],[['I1', 'I2', 'I3'], ['I1', 'I2', 'I5']]]\n",
        "     \"\"\"\n",
        "    ##########################################################################\n",
        "    #                     TODO: Implement this function                      #\n",
        "    ##########################################################################\n",
        "    # Replace \"raise NotImplementedError()\" with your code\n",
        "    \n",
        "    frequent_items = []\n",
        "    reordered_itemset = []\n",
        "    frequent_itemsets_reordered = []\n",
        "   \n",
        "    # Apriori pass 1\n",
        "    L1 = apriori_pass_1(trans_data, min_sup)\n",
        "    frequent_items.append(list(L1.keys()))\n",
        "    \n",
        "    # Handle case when K_max is less than 2\n",
        "    if K_max < 2:\n",
        "        return frequent_items\n",
        "          \n",
        "         \n",
        "    # Apriori pass 2\n",
        "    L2 = apriori_pass_2(L1, trans_data, min_sup)\n",
        "    # Convert tuple keys to lists\n",
        "    L2_as_lists = [list(itemset) for itemset in L2.keys()]\n",
        "   # L2_as_lists = [reorder_itemset(list(itemset)) for itemset in L2.keys()]\n",
        "    print(\"L2_as_lists: \" , L2_as_lists)\n",
        "    frequent_items.append(L2_as_lists) \n",
        "\n",
        "    # Handle case when K_max is less than 3\n",
        "    if K_max < 3:\n",
        "        return frequent_items\n",
        "        \n",
        "           # What I added\n",
        "           \n",
        "    # # Apriori pass 3\n",
        "    L3 = apriori_pass_3(L2, trans_data, min_sup)\n",
        "    # Convert tuple keys to lists\n",
        "    L3_as_lists = [list(itemset) for itemset in L3.keys()]\n",
        "    print(\"L3_as_lists: \" , L3_as_lists)\n",
        "    frequent_items.append(L3_as_lists)\n",
        "           \n",
        "    # Handle case when K_max is less than 4\n",
        "    if K_max < 4:\n",
        "        return frequent_items\n",
        "    \n",
        "    # Initialize Lk for pass 3\n",
        "    #Lk = L3\n",
        "\n",
        "    # Apriori pass 3\n",
        "    # k = 4\n",
        "    # #k = 3\n",
        "    # while k <= K_max:\n",
        "    #     Lk = apriori_pass_3(Lk, trans_data, min_sup)\n",
        "    #     if not Lk:\n",
        "    #         break\n",
        "    #     frequent_items.append(list(Lk.keys()))\n",
        "    #     k += 1\n",
        "        \n",
        "           # What I added end \n",
        "\n",
        "    # Initialize Lk for pass 3\n",
        "   # Lk = L2\n",
        "    Lk = L3\n",
        "\n",
        "    # Apriori pass 3\n",
        "    #k = 6\n",
        "   # k = 3\n",
        "    k = 4\n",
        "   # print(\"Entering loop\")\n",
        "    while k <= K_max:\n",
        "        #Lk = apriori_pass_3(Lk, trans_data, min_sup)\n",
        "        Lk = apriori_pass_4(Lk, trans_data, min_sup)\n",
        "        if not Lk:\n",
        "            break\n",
        "        frequent_items.append(list(Lk.keys()))\n",
        "        print(\"k:\", k, \"length of frequent_items:\", len(frequent_items))  #\n",
        "        k += 1\n",
        "        \n",
        "    frequent_itemsets_reordered = []\n",
        "    for itemset in frequent_items:\n",
        "        reordered_itemset = reorder_itemset(itemset)\n",
        "        frequent_itemsets_reordered.append(reordered_itemset)\n",
        "    \n",
        "    print(\"Final frequent itemsets after reorder:\", frequent_itemsets_reordered) \n",
        "    return frequent_itemsets_reordered\n",
        "\n",
        "\n",
        "    #raise NotImplementedError()\n",
        "\n",
        "    ###########################################################################\n",
        "    #                            END OF YOUR CODE                             #\n",
        "    ###########################################################################\n",
        "\n",
        "    #return frequent_items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H96JpQ-sohIf"
      },
      "source": [
        "**Your Answer is needed in the following cell:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 540,
      "metadata": {
        "id": "ZNVdmJHy0gEr"
      },
      "outputs": [],
      "source": [
        "# def gen_confidencePair_TOP5(trans_data : list, min_sup: int)-> list:\n",
        "#     \"\"\"\n",
        "#     Generates  Confidence pairs.\n",
        "#     You are supported to generate the (Top 5) association rules discoved in the dataset.\n",
        "#     The ranking is achieved based on their confidences.\n",
        "\n",
        "#     This function takes as input the list of frequent pairs and\n",
        "#     and returns a list of pairs with corresponding confidence.\n",
        "#     The input of the function:\n",
        "#     trans_data: (tpye:list) contains the transition dataset, for example, if we use the ''test-data.txt'' as input, then the trans_data is:\n",
        "#     [['I1', 'I2', 'I5'], ['I2', 'I4'], ['I2', 'I4'], ['I1', 'I2', 'I4'], ['I1', 'I3'], ['I2', 'I3'], ['I1', 'I3'], ['I1', 'I2', 'I3', 'I5'], ['I1', 'I2', 'I3']]\n",
        "#     You will find how to transform the 'txt' data into a list in the testing cell.\n",
        "#     min_sup: minimum support count, pay attention, this is for the support count not for support(relative support)\n",
        "\n",
        "#     For the output, if you use the dataset ''test-dataset.txt'' with min_sup = 100, then output looks like:\n",
        "#     [['FRO40251','GRO85051'], ['FRO40251','FRO92469'],['DAI62779','ELE21353'],['ELE88583','SNA24799'],['SNA53220','SNA93860']]\n",
        "#     where the list ['FRO40251' 'GRO85051'] represents the association rule {'FRO40251'->'GRO85051'}\n",
        "#     \"\"\"\n",
        "#     ##########################################################################\n",
        "#     #                     TODO: Implement this function                      #\n",
        "#     ##########################################################################\n",
        "#     #Replace \"raise NotImplementedError()\" with your code\n",
        "\n",
        "#     raise NotImplementedError()\n",
        "\n",
        "#     ###########################################################################\n",
        "#     #                            END OF YOUR CODE                             #\n",
        "#     ###########################################################################\n",
        "    \n",
        "#     return confidencePair\n",
        "\n",
        "\n",
        "def count_support(trans_data: list, itemset: set) -> int:\n",
        "    count = 0\n",
        "    for basket in trans_data:\n",
        "        if itemset.issubset(set(basket)):\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "def gen_confidencePair_TOP5(trans_data : list, min_sup: int)-> list:\n",
        "    \"\"\"\n",
        "    Generates  Confidence pairs.\n",
        "    You are supported to generate the (Top 5) association rules discoved in the dataset.\n",
        "    The ranking is achieved based on their confidences.\n",
        "\n",
        "    This function takes as input the list of frequent pairs and\n",
        "    and returns a list of pairs with corresponding confidence.\n",
        "    The input of the function:\n",
        "    trans_data: (tpye:list) contains the transition dataset, for example, if we use the ''test-data.txt'' as input, then the trans_data is:\n",
        "    [['I1', 'I2', 'I5'], ['I2', 'I4'], ['I2', 'I4'], ['I1', 'I2', 'I4'], ['I1', 'I3'], ['I2', 'I3'], ['I1', 'I3'], ['I1', 'I2', 'I3', 'I5'], ['I1', 'I2', 'I3']]\n",
        "    You will find how to transform the 'txt' data into a list in the testing cell.\n",
        "    min_sup: minimum support count, pay attention, this is for the support count not for support(relative support)\n",
        "\n",
        "    For the output, if you use the dataset ''test-dataset.txt'' with min_sup = 100, then output looks like:\n",
        "    [['FRO40251','GRO85051'], ['FRO40251','FRO92469'],['DAI62779','ELE21353'],['ELE88583','SNA24799'],['SNA53220','SNA93860']]\n",
        "    where the list ['FRO40251' 'GRO85051'] represents the association rule {'FRO40251'->'GRO85051'}\n",
        "    \"\"\"\n",
        "    ##########################################################################\n",
        "    #                     TODO: Implement this function                      #\n",
        "    ##########################################################################\n",
        "    #Replace \"raise NotImplementedError()\" with your code\n",
        "\n",
        "    frequent_pairs = apriori(trans_data, min_sup, K_max=2)\n",
        "    confidence_pairs = []\n",
        "    \n",
        "    for pair in frequent_pairs[1]:\n",
        "        X, Y = pair\n",
        "        support_XY = count_support(trans_data, set(X + Y))\n",
        "        support_X = count_support(trans_data, set(X))\n",
        "        confidence_XY = support_XY / support_X\n",
        "        confidence_pairs.append([X, Y, confidence_XY])\n",
        "    \n",
        "    confidence_pairs.sort(key=lambda x: x[2], reverse=True)\n",
        "    top5_confidence_pairs = confidence_pairs[:5]\n",
        "    \n",
        "    return [[X, Y] for X, Y, _ in top5_confidence_pairs]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33_yliDrohIg"
      },
      "source": [
        "### Test Case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 541,
      "metadata": {
        "id": "L_UiywpDUQyr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "K test:  1\n",
            "Expected frequent itemsets for size 1: ['I1', 'I2', 'I3', 'I4', 'I5']\n",
            "Converted generated frequent itemsets for size 1: ['I1', 'I2', 'I5', 'I4', 'I3']\n",
            "Discrepancy found!\n",
            "K test:  2\n",
            "Expected frequent itemsets for size 2: [['I1', 'I2'], ['I1', 'I3'], ['I1', 'I5'], ['I2', 'I3'], ['I2', 'I4'], ['I2', 'I5']]\n",
            "Converted generated frequent itemsets for size 2: [['I1', 'I2'], ['I1', 'I3'], ['I1', 'I5'], ['I2', 'I3'], ['I2', 'I4'], ['I2', 'I5']]\n",
            "Itemsets match.\n",
            "K test:  3\n",
            "Expected frequent itemsets for size 3: [['I1', 'I2', 'I3'], ['I1', 'I2', 'I5']]\n",
            "Converted generated frequent itemsets for size 3: [['I1', 'I2', 'I3'], ['I1', 'I2', 'I5']]\n",
            "Itemsets match.\n",
            "YOU PASSED THE TEST!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "The following codes will be used for testing your results.\n",
        "\"\"\"\n",
        "#1. Test the frequent itemsets for the test data\n",
        "\n",
        "#Loading dataset and convert the data into a list of transaction data\n",
        "file_path = './test-dataset.txt'\n",
        "file = open(file_path, \"r\")\n",
        "trans_data = []\n",
        "while True:\n",
        "  line = file.readline()\n",
        "  # END OF FILE IS REACHED\n",
        "  if not line:\n",
        "    file.close()\n",
        "    break\n",
        "\n",
        "  # SPLITTING THE ITEMS\n",
        "  basket = line.split(\" \")\n",
        "  basket.pop()    #FOR REMOVING '\\n'\n",
        "  trans_data.append(list(basket))\n",
        "# Begin Test\n",
        "# The ground truth\n",
        "frequent_items_testing = [['I1', 'I2', 'I3', 'I4', 'I5'],[['I1', 'I2'], ['I1', 'I3'], ['I1', 'I5'], ['I2', 'I3'], ['I2', 'I4'], ['I2', 'I5']],[['I1', 'I2', 'I3'], ['I1', 'I2', 'I5']]]\n",
        "min_sup = 2\n",
        "K_max = 3\n",
        "# min_sup = 15\n",
        "# K_max = 5\n",
        "frequent_items = apriori(trans_data,min_sup,K_max)\n",
        "\n",
        "# Compare converted generated frequent itemsets with expected frequent itemsets\n",
        "for k, itemset_list in enumerate(frequent_items_testing):\n",
        "\n",
        "#for k in range(min(K_max, len(frequent_items_testing))):\n",
        "    itemset_list = frequent_items_testing[k]\n",
        "    print(\"K test: \", k + 1)\n",
        "    print(f\"Expected frequent itemsets for size {k + 1}: {itemset_list}\")\n",
        "    print(f\"Converted generated frequent itemsets for size {k + 1}: {frequent_items[k]}\")\n",
        "    if itemset_list != frequent_items[k]:\n",
        "        print(\"Discrepancy found!\")\n",
        "    else:\n",
        "        print(\"Itemsets match.\")\n",
        "\n",
        "for k in range(K_max):\n",
        "  Lk = frequent_items[k]\n",
        "  Lktest = frequent_items_testing[k]\n",
        "  if k == 0:\n",
        "    assert sorted(Lk)==sorted(Lktest), f\"Frequent Itemset with size {k+1} is not correct\"\n",
        "  else:\n",
        "    for item in Lk:\n",
        "      if sorted(item) in Lktest:\n",
        "        Lktest.remove(sorted(item))\n",
        "    assert len(Lktest)==0, f\"Frequent Itemset with size {k+1} is not correct\"\n",
        "print(\"YOU PASSED THE TEST!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 543,
      "metadata": {
        "id": "Xg2SWvycgfLu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "K test:  1\n",
            "Expected frequent itemsets for size 1: ['DAI62779', 'DAI75645', 'ELE17451', 'FRO40251']\n",
            "Converted generated frequent itemsets for size 1: []\n",
            "Discrepancy found!\n",
            "K test:  2\n",
            "Expected frequent itemsets for size 2: ['DAI62779', 'DAI75645', 'ELE17451', 'SNA80324']\n",
            "Converted generated frequent itemsets for size 2: []\n",
            "Discrepancy found!\n",
            "K test:  3\n",
            "Expected frequent itemsets for size 3: ['DAI62779', 'DAI75645', 'FRO40251', 'SNA80324']\n",
            "Converted generated frequent itemsets for size 3: []\n",
            "Discrepancy found!\n",
            "K test:  4\n",
            "Expected frequent itemsets for size 4: ['DAI62779', 'ELE17451', 'FRO40251', 'GRO85051']\n",
            "Converted generated frequent itemsets for size 4: []\n",
            "Discrepancy found!\n",
            "K test:  5\n",
            "Expected frequent itemsets for size 5: ['DAI62779', 'ELE17451', 'FRO40251', 'SNA80324']\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[543], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK test: \u001b[39m\u001b[38;5;124m\"\u001b[39m, k \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected frequent itemsets for size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitemset_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverted generated frequent itemsets for size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mfrequent_items\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m itemset_list \u001b[38;5;241m!=\u001b[39m frequent_items[k]:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDiscrepancy found!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "#2. Test the frequent itemsets for the browsing-dataset\n",
        "#Loading dataset and convert the data into a list of transaction data\n",
        "file_path = './browsing-dataset.txt'\n",
        "file = open(file_path, \"r\")\n",
        "trans_data = []\n",
        "while True:\n",
        "  line = file.readline()\n",
        "  # END OF FILE IS REACHED\n",
        "  if not line:\n",
        "    file.close()\n",
        "    break\n",
        "  # SPLITTING THE ITEMS\n",
        "  basket = line.split(\" \")\n",
        "  basket.pop()    #FOR REMOVING '\\n'\n",
        "  trans_data.append(list(basket))\n",
        "# Begin Test\n",
        "# The ground truth\n",
        "frequent_items_testing = [['DAI62779', 'DAI75645', 'ELE17451', 'FRO40251'], ['DAI62779', 'DAI75645', 'ELE17451', 'SNA80324'], ['DAI62779', 'DAI75645', 'FRO40251', 'SNA80324'], ['DAI62779', 'ELE17451', 'FRO40251', 'GRO85051'], ['DAI62779', 'ELE17451', 'FRO40251', 'SNA80324'], ['DAI62779', 'ELE17451', 'GRO85051', 'SNA80324'], ['DAI62779', 'FRO19221', 'SNA53220', 'SNA93860'], ['DAI62779', 'FRO40251', 'GRO85051', 'SNA80324'], ['DAI75645', 'ELE17451', 'FRO40251', 'SNA80324'], ['DAI75645', 'FRO40251', 'GRO85051', 'SNA80324'], ['ELE17451', 'FRO40251', 'GRO85051', 'SNA80324']]\n",
        "# min_sup = 15\n",
        "# K_max = 5\n",
        "min_sup = 100\n",
        "K_max = 4\n",
        "frequent_items = apriori(trans_data,min_sup,K_max)\n",
        "        \n",
        "# Compare converted generated frequent itemsets with expected frequent itemsets\n",
        "for k, itemset_list in enumerate(frequent_items_testing):\n",
        "\n",
        "#for k in range(min(K_max, len(frequent_items_testing))):\n",
        "    itemset_list = frequent_items_testing[k]\n",
        "    print(\"K test: \", k + 1)\n",
        "    print(f\"Expected frequent itemsets for size {k + 1}: {itemset_list}\")\n",
        "    print(f\"Converted generated frequent itemsets for size {k + 1}: {frequent_items[k]}\")\n",
        "    if itemset_list != frequent_items[k]:\n",
        "        print(\"Discrepancy found!\")\n",
        "    else:\n",
        "        print(\"Itemsets match.\")\n",
        "        \n",
        "#For simplifity, we only check the case for L4\n",
        "Lk = frequent_items[K_max-1]\n",
        "Lktest = frequent_items_testing\n",
        "for item in Lk:\n",
        "  if sorted(item) in Lktest:\n",
        "    Lktest.remove(sorted(item))\n",
        "assert len(Lktest)==0, f\"Frequent Itemset with size {K_max} is not correct\"\n",
        "print(\"YOU PASSED THE TEST!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lo4xE3FriVlj"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "The TOP5 Associations Rules You Discovered are not correct!",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[529], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(item) \u001b[38;5;129;01min\u001b[39;00m association_rules_testing:\n\u001b[1;32m     26\u001b[0m     association_rules_testing\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;28msorted\u001b[39m(item))\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(association_rules_testing)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TOP5 Associations Rules You Discovered are not correct!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYOU PASSED THE TEST!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mAssertionError\u001b[0m: The TOP5 Associations Rules You Discovered are not correct!"
          ]
        }
      ],
      "source": [
        "#3. Test the TOP 5 association rule\n",
        "#Loading dataset and convert the data into a list of transaction data\n",
        "file_path = './browsing-dataset.txt'\n",
        "file = open(file_path, \"r\")\n",
        "trans_data = []\n",
        "while True:\n",
        "  line = file.readline()\n",
        "  # END OF FILE IS REACHED\n",
        "  if not line:\n",
        "    file.close()\n",
        "    break\n",
        "  # SPLITTING THE ITEMS\n",
        "  basket = line.split(\" \")\n",
        "  basket.pop()    #FOR REMOVING '\\n'\n",
        "  trans_data.append(list(basket))\n",
        "# Begin Test\n",
        "# The ground truth\n",
        "association_rules_testing = [['DAI93865', 'FRO40251'], ['FRO40251', 'GRO85051'], ['FRO40251', 'GRO38636'], ['ELE12951', 'FRO40251'], ['DAI88079', 'FRO40251']]\n",
        "\n",
        "min_sup = 100\n",
        "association_rules = gen_confidencePair_TOP5(trans_data,min_sup)\n",
        "#print(f'association_rules: {association_rules}')\n",
        "\n",
        "for item in association_rules:\n",
        "  if sorted(item) in association_rules_testing:\n",
        "    association_rules_testing.remove(sorted(item))\n",
        "assert len(association_rules_testing)==0, f\"The TOP5 Associations Rules You Discovered are not correct!\"\n",
        "print(\"YOU PASSED THE TEST!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "emmanuel",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
